#!/usr/bin/env python3
"""
Run the semantic-aware experiment and capture results.
"""

import json
import time
import os
import numpy as np
import math
from datetime import datetime

# Simplified implementations
class Card:
    def __init__(self, difficulty=0.3):
        self.ease_factor = 2.5
        self.interval = 1
        self.repetitions = 0
        self.difficulty = difficulty

class SyntheticLearner:
    def __init__(self):
        self.memory_strength = np.random.lognormal(0, 0.3)
        self.forgetting_rate = np.random.gamma(2, 0.05)
        self.consistency = np.random.beta(4, 1) * 0.4 + 0.6
        
    def simulate_review(self, card, days_since_review):
        base_retention = math.exp(-self.forgetting_rate * max(1, days_since_review) / 10)
        adjusted_retention = base_retention * self.memory_strength
        performance = adjusted_retention + np.random.normal(0, 0.1 * (1 - self.consistency))
        performance = max(0, min(1, performance))
        
        if performance < 0.3:
            return 1
        elif performance < 0.6:
            return 2
        elif performance < 0.85:
            return 3
        else:
            return 4

def jaccard_similarity(str1, str2):
    words1 = set(str1.lower().split())
    words2 = set(str2.lower().split())
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    return len(intersection) / len(union) if len(union) > 0 else 0

def create_content_domains():
    medical_items = [
        "Myocardial infarction - death of heart muscle tissue",
        "Cardiac arrest - sudden loss of heart function", 
        "Angina pectoris - chest pain from reduced blood flow",
        "Arrhythmia - irregular heart rhythm",
        "Bradycardia - slow heart rate below 60 bpm",
        "Tachycardia - fast heart rate above 100 bpm",
        "Hypertension - elevated blood pressure",
        "Hypotension - low blood pressure"
    ]
    
    programming_items = [
        "Recursion - function calls itself",
        "Algorithm - step-by-step problem solving procedure",
        "Variable - storage location with symbolic name",
        "Loop - repeated execution of code block",
        "Array - collection of elements at indices",
        "Hash table - key-value data structure",
        "Binary tree - tree with max two children",
        "Queue - first-in-first-out data structure"
    ]
    
    random_items = [
        "Serendipity - pleasant surprise discovery",
        "Telescope - instrument for viewing distant objects",
        "Bicycle - two-wheeled vehicle with pedals",
        "Symphony - large-scale orchestral composition",
        "Archaeology - study of human history through artifacts",
        "Photosynthesis - plants convert light to chemical energy",
        "Democracy - government by the people",
        "Metamorphosis - biological transformation process"
    ]
    
    domains = {
        'Medical': {'items': medical_items, 'density': 'high'},
        'Programming': {'items': programming_items, 'density': 'medium'},
        'General': {'items': random_items, 'density': 'low'}
    }
    
    for domain_name, domain_data in domains.items():
        items = domain_data['items']
        similarity_matrix = np.zeros((len(items), len(items)))
        
        for i in range(len(items)):
            for j in range(len(items)):
                if i != j:
                    similarity_matrix[i][j] = jaccard_similarity(items[i], items[j])
        
        domain_data['similarity_matrix'] = similarity_matrix
        
        clusters = []
        for i, item in enumerate(items):
            max_sim_idx = np.argmax(similarity_matrix[i])
            clusters.append(max_sim_idx % 3)
        domain_data['clusters'] = clusters
    
    return domains

def apply_semantic_scheduling(card_idx, similarity_matrix, clusters, semantic_params):
    similarity_scores = similarity_matrix[card_idx]
    cluster_id = clusters[card_idx]
    interval_modifier = 1.0
    
    if 'interference_strength' in semantic_params:
        max_similarity = np.max(similarity_scores)
        interference = semantic_params['interference_strength'] * max_similarity
        interval_modifier *= (1.0 - interference)
    
    if 'cluster_weight' in semantic_params:
        cluster_adjustment = semantic_params['cluster_weight']
        if cluster_id == 0:
            interval_modifier *= (1.0 + cluster_adjustment)
        else:
            interval_modifier *= (1.0 - cluster_adjustment * 0.5)
    
    if 'similarity_boost' in semantic_params:
        avg_similarity = np.mean(similarity_scores)
        boost = semantic_params['similarity_boost'] * avg_similarity
        interval_modifier *= (1.0 + boost)
    
    return max(0.1, interval_modifier)

def evaluate_semantic_algorithm(domain_data, semantic_params):
    items = domain_data['items']
    similarity_matrix = domain_data['similarity_matrix']
    clusters = domain_data['clusters']
    
    learners = [SyntheticLearner() for _ in range(8)]
    cards = [Card(difficulty=np.random.uniform(0.2, 0.8)) for _ in range(len(items))]
    
    total_success = 0
    n_reviews = 0
    
    for learner in learners:
        for day in range(20):
            for card_idx, card in enumerate(cards):
                modifier = apply_semantic_scheduling(card_idx, similarity_matrix, clusters, semantic_params)
                modified_interval = max(1, int(card.interval * modifier))
                
                if day % modified_interval == 0:
                    grade = learner.simulate_review(card, day)
                    success = 1 if grade >= 3 else 0
                    total_success += success
                    n_reviews += 1
                    
                    if success:
                        card.interval = max(1, int(card.interval * 1.5))
                    else:
                        card.interval = 1
    
    if n_reviews > 0:
        success_rate = total_success / n_reviews
        efficiency = success_rate / max(1, n_reviews / 10)
    else:
        success_rate = 0
        efficiency = 0
    
    return {
        'efficiency': efficiency,
        'success_rate': success_rate,
        'n_reviews': n_reviews
    }

def run_baseline_comparison(domains):
    baseline_results = {}
    
    for domain_name, domain_data in domains.items():
        items = domain_data['items']
        learners = [SyntheticLearner() for _ in range(8)]
        cards = [Card(difficulty=np.random.uniform(0.2, 0.8)) for _ in range(len(items))]
        
        total_success = 0
        n_reviews = 0
        
        for learner in learners:
            for day in range(20):
                for card in cards:
                    if day % max(1, card.interval) == 0:
                        grade = learner.simulate_review(card, day)
                        success = 1 if grade >= 3 else 0
                        total_success += success
                        n_reviews += 1
                        
                        if success:
                            card.interval = max(1, int(card.interval * 1.5))
                        else:
                            card.interval = 1
        
        success_rate = total_success / n_reviews if n_reviews > 0 else 0
        efficiency = success_rate / max(1, n_reviews / 10)
        
        baseline_results[domain_name] = {
            'efficiency': efficiency,
            'success_rate': success_rate,
            'n_reviews': n_reviews
        }
    
    return baseline_results

def run_semantic_evolution(domains):
    population_size = 15
    generations = 8
    
    best_algorithms = []
    
    for gen in range(generations):
        population = []
        for i in range(population_size):
            params = {
                'interference_strength': np.random.uniform(0.1, 0.5),
                'cluster_weight': np.random.uniform(0.1, 0.4),
                'similarity_boost': np.random.uniform(0.1, 0.3)
            }
            population.append(params)
        
        generation_results = []
        for params in population:
            domain_scores = []
            for domain_name, domain_data in domains.items():
                result = evaluate_semantic_algorithm(domain_data, params)
                domain_scores.append(result['efficiency'])
            
            avg_fitness = np.mean(domain_scores)
            generation_results.append({
                'params': params,
                'fitness': avg_fitness,
                'domain_scores': domain_scores
            })
        
        generation_results.sort(key=lambda x: x['fitness'], reverse=True)
        best_algorithms = generation_results[:3]
    
    return best_algorithms

def main():
    print("=== Semantic-Aware Algorithm Discovery Experiment ===")
    start_time = time.time()
    
    # Set random seed for reproducibility
    np.random.seed(42)
    
    domains = create_content_domains()
    
    print("\nDomain Statistics:")
    for domain_name, domain_data in domains.items():
        avg_sim = np.mean(domain_data['similarity_matrix'][np.triu_indices_from(domain_data['similarity_matrix'], k=1)])
        print(f"  {domain_name}: {len(domain_data['items'])} items, avg similarity: {avg_sim:.3f}")
    
    print("\nRunning baseline evaluation...")
    baseline_results = run_baseline_comparison(domains)
    
    print("Running semantic evolution...")
    evolved_results = run_semantic_evolution(domains)
    
    # Calculate improvements
    improvements = {}
    for domain_name in domains.keys():
        baseline_eff = baseline_results[domain_name]['efficiency']
        # Get domain-specific score from best algorithm
        best_alg = evolved_results[0]
        domain_idx = list(domains.keys()).index(domain_name)
        semantic_eff = best_alg['domain_scores'][domain_idx]
        
        improvement = (semantic_eff - baseline_eff) / baseline_eff if baseline_eff > 0 else 0
        improvements[domain_name] = improvement
    
    # Generate report
    report = {
        'experiment_metadata': {
            'experiment_id': 'exp_semantic_aware_002',
            'title': 'Semantic-Aware Algorithm Discovery',
            'execution_date': datetime.now().isoformat(),
            'duration_minutes': (time.time() - start_time) / 60.0,
            'hypothesis_tested': 'Semantic-aware algorithms achieve â‰¥15% improvement on high-similarity content'
        },
        'domain_analysis': {
            domain_name: {
                'semantic_density': domain_data['density'],
                'n_items': len(domain_data['items']),
                'avg_similarity': float(np.mean(domain_data['similarity_matrix'][np.triu_indices_from(domain_data['similarity_matrix'], k=1)]))
            } for domain_name, domain_data in domains.items()
        },
        'baseline_results': baseline_results,
        'evolutionary_results': {
            'generations': 8,
            'population_size': 15,
            'top_fitness_scores': [alg['fitness'] for alg in evolved_results]
        },
        'performance_improvements': improvements,
        'key_findings': {
            'hypothesis_supported': any(imp >= 0.15 for imp in improvements.values()),
            'best_improvement_domain': max(improvements.keys(), key=lambda k: improvements[k]) if improvements else None,
            'best_improvement_pct': max(improvements.values()) * 100 if improvements else 0,
            'semantic_operations_discovered': ['interference_reduction', 'cluster_spacing', 'similarity_boost']
        },
        'top_algorithm_analysis': {
            'best_algorithm_params': evolved_results[0]['params'],
            'fitness_score': evolved_results[0]['fitness']
        }
    }
    
    # Save results
    os.makedirs('data/exp_semantic_aware_002', exist_ok=True)
    
    with open('data/exp_semantic_aware_002/experiment_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\n=== EXPERIMENT COMPLETE ===")
    print(f"Execution time: {report['experiment_metadata']['duration_minutes']:.1f} minutes")
    print(f"Hypothesis supported: {report['key_findings']['hypothesis_supported']}")
    if improvements:
        print(f"Best improvement: {report['key_findings']['best_improvement_pct']:.1f}% on {report['key_findings']['best_improvement_domain']}")
    
    print("\nDomain-specific improvements:")
    for domain, improvement in improvements.items():
        print(f"  {domain}: {improvement*100:.1f}%")
    
    print("Top algorithm parameters:")
    for param, value in evolved_results[0]['params'].items():
        print(f"  {param}: {value:.3f}")
    
    return report

if __name__ == "__main__":
    report = main()