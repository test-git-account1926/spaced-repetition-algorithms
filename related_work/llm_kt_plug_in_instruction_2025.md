# LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction (2025)

## CS197 Analysis Framework

### Problem
Knowledge tracing (KT) aims to predict student performance on future questions based on historical question-answer records, but traditional sequence-based models fail to leverage the rich world knowledge about questions that LLMs possess for reasoning about student behavioral patterns.

### Prior Assumptions in Prior Work
- Sequence interaction models with ID-based or simple textual information are sufficient for knowledge tracing
- Student behavioral patterns can be captured without external world knowledge or semantic reasoning
- Traditional KT models can adequately model student understanding without incorporating broad domain knowledge

### Insight
Large language models possess rich world knowledge and powerful reasoning capabilities that can be aligned with knowledge tracing through carefully designed instructions, creating hybrid systems that combine the strengths of LLMs with traditional sequence interaction models.

### Technical Overview
**LLM-KT Framework** with two-level alignment:
- **Task-level alignment**: Plug-and-Play instruction system that aligns LLMs with KT objectives
- **Modality-level alignment**: Integration of multiple modalities learned by traditional methods:
  - Plug-in context using compressed context embedding with question-specific and concept-specific tokens
  - Plug-in sequence using sequence adapter to enhance LLMs with traditional sequence interaction behavior

### Proof/Evaluation
- Evaluated against ~20 strong baseline methods on 4 typical KT datasets
- Achieves state-of-the-art performance across all datasets
- Comprehensive comparison demonstrates consistent improvements over existing approaches

### Impact
- **Bit Flip**: Traditional sequence models are insufficient â†’ LLM-enhanced KT systems can leverage world knowledge for superior student modeling
- Establishes framework for incorporating large language models into educational prediction tasks
- Demonstrates plug-and-play approach that can enhance existing KT systems
- Opens new research direction combining foundation models with specialized educational algorithms

### CS197 Significance
This represents a fundamental shift from domain-specific algorithms to foundation model integration in educational technology, suggesting LLMs can serve as knowledge-rich backends for educational prediction tasks while maintaining compatibility with existing specialized methods.

**Citation**: Wang, Z., Zhou, J., Chen, Q., Zhang, M., Jiang, B., Zhou, A., Bai, Q., & He, L. (2025). LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction. arXiv preprint arXiv:2502.02945.