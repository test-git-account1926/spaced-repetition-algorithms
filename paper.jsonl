{"id":"paper_2025080900001","title":"LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning","authors":"Jiahao Zhao","journal":"ArXiv","year":"2024","doi":"2508.03275v1","url":"https://arxiv.org/html/2508.03275v1","keyAssumptions":"Items can be scheduled independently without semantic relationships; Temporal spacing alone is sufficient for optimal learning; Traditional algorithms work across all domains","citation":"Zhao, J. (2024). LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning. arXiv preprint arXiv:2508.03275.","notes":"First algorithm addressing semantic interference through LLM-powered similarity assessment. Achieves 90.2% vs 88.4% success rate. Addresses critical semantic confusion in vocabulary learning.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900002","title":"Human-like Forgetting Curves in Deep Neural Networks","authors":"Dylan Kline","journal":"ArXiv","year":"2025","doi":"2506.12034v2","url":"https://arxiv.org/abs/2506.12034","keyAssumptions":"Neural network forgetting is fundamentally different from human forgetting; Catastrophic forgetting requires specialized architectures; Human memory research has limited AI applicability","citation":"Kline, D. (2025). Human-like Forgetting Curves in Deep Neural Networks. arXiv preprint arXiv:2506.12034.","notes":"Demonstrates neural networks exhibit human-like exponential forgetting patterns. Enables direct application of spaced repetition to mitigate catastrophic forgetting. Bridges cognitive science and AI.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900003","title":"Optimizing Human Learning","authors":"Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schölkopf, Manuel Gomez-Rodriguez","journal":"ArXiv","year":"2017","doi":"1712.01856","url":"https://ar5iv.labs.arxiv.org/html/1712.01856","keyAssumptions":"Heuristic scheduling rules are sufficient; Optimal scheduling cannot be derived mathematically; Memory models too complex for optimization","citation":"Tabibian, B., Upadhyay, U., De, A., Zarezade, A., Schölkopf, B., & Gomez-Rodriguez, M. (2017). Optimizing human learning. arXiv preprint arXiv:1712.01856.","notes":"First theoretical proof that optimal spaced repetition schedules are determined by recall probability. Uses marked temporal point processes framework. Validated on Duolingo data.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900004","title":"Algorithm SM-18","authors":"Piotr Wozniak","journal":"SuperMemo.guru","year":"2019","doi":"","url":"https://supermemo.guru/wiki/Algorithm_SM-18","keyAssumptions":"Item difficulty is constant throughout learning; Single memory strength parameter sufficient; Fixed mathematical formulas for intervals","citation":"Wozniak, P. (2019). Algorithm SM-18. SuperMemo.guru. Retrieved from https://supermemo.guru/wiki/Algorithm_SM-18","notes":"Latest SuperMemo algorithm with dynamic difficulty estimation. Replaces assumption of constant item difficulty with adaptive modeling. Part of 34-year algorithm evolution achieving 1.1 to 35.3 improvement ratio over SM-2.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900005","title":"Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students","authors":"Prital Bamnodkar","journal":"ArXiv","year":"2025","doi":"2507.21109","url":"https://arxiv.org/html/2507.21109","keyAssumptions":"Experience replay alone sufficient for continual learning; Random memory sampling is optimal; Human learning strategies don't apply to neural networks","citation":"Bamnodkar, P. (2025). Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students. arXiv preprint arXiv:2507.21109.","notes":"Applies Active Recall, Deliberate Practice, and Spaced Repetition to continual learning. Achieves 13.17% vs 7.40% on Split CIFAR-100. Demonstrates human learning principles enhance neural network training.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900006","title":"Evolvable Psychology Informed Neural Network for Memory Behavior Modeling","authors":"Xiaoxuan Shen, Zhihai Hu, Qirong Chen, Shengyingjie Liu, Ruxia Liang, Jianwen Sun","journal":"ArXiv","year":"2023","doi":"2408.14492v1","url":"https://arxiv.org/html/2408.14492v1","keyAssumptions":"Memory equations have fixed descriptors; Pure mathematical or data-driven approaches are optimal; Classical memory theories cannot be improved through ML","citation":"Shen, X., Hu, Z., Chen, Q., Liu, S., Liang, R., & Sun, J. (2023). Evolvable Psychology Informed Neural Network for Memory Behavior Modeling. arXiv preprint arXiv:2408.14492.","notes":"Combines neural networks with differentiating sparse regression to evolve memory equation descriptors. Addresses controversies in memory equation formulation. Shows AI can enhance traditional psychological models.","addedDate":"2025-08-09T20:57:00Z"}