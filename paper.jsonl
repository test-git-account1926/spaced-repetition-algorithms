{"id":"paper_2025080900001","title":"LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning","authors":"Jiahao Zhao","journal":"ArXiv","year":"2024","doi":"2508.03275v1","url":"https://arxiv.org/html/2508.03275v1","keyAssumptions":"Items can be scheduled independently without semantic relationships; Temporal spacing alone is sufficient for optimal learning; Traditional algorithms work across all domains","citation":"Zhao, J. (2024). LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning. arXiv preprint arXiv:2508.03275.","notes":"First algorithm addressing semantic interference through LLM-powered similarity assessment. Achieves 90.2% vs 88.4% success rate. Addresses critical semantic confusion in vocabulary learning.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900002","title":"Human-like Forgetting Curves in Deep Neural Networks","authors":"Dylan Kline","journal":"ArXiv","year":"2025","doi":"2506.12034v2","url":"https://arxiv.org/abs/2506.12034","keyAssumptions":"Neural network forgetting is fundamentally different from human forgetting; Catastrophic forgetting requires specialized architectures; Human memory research has limited AI applicability","citation":"Kline, D. (2025). Human-like Forgetting Curves in Deep Neural Networks. arXiv preprint arXiv:2506.12034.","notes":"Demonstrates neural networks exhibit human-like exponential forgetting patterns. Enables direct application of spaced repetition to mitigate catastrophic forgetting. Bridges cognitive science and AI.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900003","title":"Optimizing Human Learning","authors":"Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schölkopf, Manuel Gomez-Rodriguez","journal":"ArXiv","year":"2017","doi":"1712.01856","url":"https://ar5iv.labs.arxiv.org/html/1712.01856","keyAssumptions":"Heuristic scheduling rules are sufficient; Optimal scheduling cannot be derived mathematically; Memory models too complex for optimization","citation":"Tabibian, B., Upadhyay, U., De, A., Zarezade, A., Schölkopf, B., & Gomez-Rodriguez, M. (2017). Optimizing human learning. arXiv preprint arXiv:1712.01856.","notes":"First theoretical proof that optimal spaced repetition schedules are determined by recall probability. Uses marked temporal point processes framework. Validated on Duolingo data.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900004","title":"Algorithm SM-18","authors":"Piotr Wozniak","journal":"SuperMemo.guru","year":"2019","doi":"","url":"https://supermemo.guru/wiki/Algorithm_SM-18","keyAssumptions":"Item difficulty is constant throughout learning; Single memory strength parameter sufficient; Fixed mathematical formulas for intervals","citation":"Wozniak, P. (2019). Algorithm SM-18. SuperMemo.guru. Retrieved from https://supermemo.guru/wiki/Algorithm_SM-18","notes":"Latest SuperMemo algorithm with dynamic difficulty estimation. Replaces assumption of constant item difficulty with adaptive modeling. Part of 34-year algorithm evolution achieving 1.1 to 35.3 improvement ratio over SM-2.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900005","title":"Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students","authors":"Prital Bamnodkar","journal":"ArXiv","year":"2025","doi":"2507.21109","url":"https://arxiv.org/html/2507.21109","keyAssumptions":"Experience replay alone sufficient for continual learning; Random memory sampling is optimal; Human learning strategies don't apply to neural networks","citation":"Bamnodkar, P. (2025). Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students. arXiv preprint arXiv:2507.21109.","notes":"Applies Active Recall, Deliberate Practice, and Spaced Repetition to continual learning. Achieves 13.17% vs 7.40% on Split CIFAR-100. Demonstrates human learning principles enhance neural network training.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900006","title":"Evolvable Psychology Informed Neural Network for Memory Behavior Modeling","authors":"Xiaoxuan Shen, Zhihai Hu, Qirong Chen, Shengyingjie Liu, Ruxia Liang, Jianwen Sun","journal":"ArXiv","year":"2023","doi":"2408.14492v1","url":"https://arxiv.org/html/2408.14492v1","keyAssumptions":"Memory equations have fixed descriptors; Pure mathematical or data-driven approaches are optimal; Classical memory theories cannot be improved through ML","citation":"Shen, X., Hu, Z., Chen, Q., Liu, S., Liang, R., & Sun, J. (2023). Evolvable Psychology Informed Neural Network for Memory Behavior Modeling. arXiv preprint arXiv:2408.14492.","notes":"Combines neural networks with differentiating sparse regression to evolve memory equation descriptors. Addresses controversies in memory equation formulation. Shows AI can enhance traditional psychological models.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025081100007","title":"Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review","authors":"Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong","journal":"ArXiv","year":"2025","doi":"2409.06131v2","url":"https://arxiv.org/html/2409.06131v2","keyAssumptions":"Random sampling is optimal for LLM training; All training data should receive equal attention; Uniform data distribution leads to best performance","citation":"Prakriya, N., Yen, J. N., Hsieh, C. J., & Cong, J. (2025). Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review. arXiv preprint arXiv:2409.06131.","notes":"Applies spaced repetition principles to LLM pretraining. Achieves equivalent performance with 5-19% of training tokens. Demonstrates 2× parameter efficiency. Revolutionary application of memory principles to foundation models.","addedDate":"2025-08-11T18:26:00Z"}
{"id":"paper_2025081100008","title":"Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing","authors":"Grey Kuling, Marinka Zitnik","journal":"ArXiv","year":"2025","doi":"2507.00032","url":"https://arxiv.org/abs/2507.00032","keyAssumptions":"Large datasets required for knowledge tracing; Gradient backpropagation through time necessary; Raw data storage essential for memory systems","citation":"Kuling, G., & Zitnik, M. (2025). Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing. arXiv preprint arXiv:2507.00032.","notes":"Biologically-inspired dual memory system combining Hebbian updates with gradient consolidation. Achieves 99.01% memory reduction while outperforming baselines. Demonstrates few-shot personalization.","addedDate":"2025-08-11T18:26:00Z"}
{"id":"paper_2025081100009","title":"DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Scheduling","authors":"Jing Wang, et al.","journal":"MDPI Applied Sciences","year":"2024","doi":"10.3390/app14135591","url":"https://www.mdpi.com/2076-3417/14/13/5591","keyAssumptions":"Heuristic scheduling rules are sufficient; Static mathematical formulas can capture learning dynamics; Optimal policies don't require continuous learning","citation":"Wang, J., et al. (2024). DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Scheduling. Applied Sciences, 14(13), 5591.","notes":"Treats spaced repetition as sequential decision-making problem. Uses deep reinforcement learning for dynamic scheduling optimization. Demonstrates adaptive policy learning for personalized review scheduling.","addedDate":"2025-08-11T18:26:00Z"}
{"id":"paper_2025081100010","title":"FSRS Benchmark: Large-scale Evaluation of Spaced Repetition Algorithms","authors":"Jarrett Ye, L-M-Sherlock, et al.","journal":"Open Source","year":"2023-2025","doi":"","url":"https://github.com/open-spaced-repetition/srs-benchmark","keyAssumptions":"Algorithm evaluation could use ML metrics; Small proprietary datasets sufficient; SuperMemo dominance unquestionable","citation":"Ye, J., et al. (2023-2025). FSRS Benchmark: Large-scale Evaluation of Spaced Repetition Algorithms. GitHub/Hugging Face.","notes":"Created largest open-source SRS evaluation dataset (727M+ reviews). Developed anti-cheating RMSE(bins) metric. Sparked algorithm wars between FSRS and SuperMemo. Democratized SRS research.","addedDate":"2025-08-11T18:26:00Z"}