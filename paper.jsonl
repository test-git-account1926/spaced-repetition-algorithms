{"id":"paper_2025080900001","title":"LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning","authors":"Jiahao Zhao","journal":"ArXiv","year":"2024","doi":"2508.03275v1","url":"https://arxiv.org/html/2508.03275v1","keyAssumptions":"Items can be scheduled independently without semantic relationships; Temporal spacing alone is sufficient for optimal learning; Traditional algorithms work across all domains","citation":"Zhao, J. (2024). LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning. arXiv preprint arXiv:2508.03275.","notes":"First algorithm addressing semantic interference through LLM-powered similarity assessment. Achieves 90.2% vs 88.4% success rate. Addresses critical semantic confusion in vocabulary learning.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900002","title":"Human-like Forgetting Curves in Deep Neural Networks","authors":"Dylan Kline","journal":"ArXiv","year":"2025","doi":"2506.12034v2","url":"https://arxiv.org/abs/2506.12034","keyAssumptions":"Neural network forgetting is fundamentally different from human forgetting; Catastrophic forgetting requires specialized architectures; Human memory research has limited AI applicability","citation":"Kline, D. (2025). Human-like Forgetting Curves in Deep Neural Networks. arXiv preprint arXiv:2506.12034.","notes":"Demonstrates neural networks exhibit human-like exponential forgetting patterns. Enables direct application of spaced repetition to mitigate catastrophic forgetting. Bridges cognitive science and AI.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900003","title":"Optimizing Human Learning","authors":"Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schölkopf, Manuel Gomez-Rodriguez","journal":"ArXiv","year":"2017","doi":"1712.01856","url":"https://ar5iv.labs.arxiv.org/html/1712.01856","keyAssumptions":"Heuristic scheduling rules are sufficient; Optimal scheduling cannot be derived mathematically; Memory models too complex for optimization","citation":"Tabibian, B., Upadhyay, U., De, A., Zarezade, A., Schölkopf, B., & Gomez-Rodriguez, M. (2017). Optimizing human learning. arXiv preprint arXiv:1712.01856.","notes":"First theoretical proof that optimal spaced repetition schedules are determined by recall probability. Uses marked temporal point processes framework. Validated on Duolingo data.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900004","title":"Algorithm SM-18","authors":"Piotr Wozniak","journal":"SuperMemo.guru","year":"2019","doi":"","url":"https://supermemo.guru/wiki/Algorithm_SM-18","keyAssumptions":"Item difficulty is constant throughout learning; Single memory strength parameter sufficient; Fixed mathematical formulas for intervals","citation":"Wozniak, P. (2019). Algorithm SM-18. SuperMemo.guru. Retrieved from https://supermemo.guru/wiki/Algorithm_SM-18","notes":"Latest SuperMemo algorithm with dynamic difficulty estimation. Replaces assumption of constant item difficulty with adaptive modeling. Part of 34-year algorithm evolution achieving 1.1 to 35.3 improvement ratio over SM-2.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900005","title":"Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students","authors":"Prital Bamnodkar","journal":"ArXiv","year":"2025","doi":"2507.21109","url":"https://arxiv.org/html/2507.21109","keyAssumptions":"Experience replay alone sufficient for continual learning; Random memory sampling is optimal; Human learning strategies don't apply to neural networks","citation":"Bamnodkar, P. (2025). Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students. arXiv preprint arXiv:2507.21109.","notes":"Applies Active Recall, Deliberate Practice, and Spaced Repetition to continual learning. Achieves 13.17% vs 7.40% on Split CIFAR-100. Demonstrates human learning principles enhance neural network training.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900006","title":"Evolvable Psychology Informed Neural Network for Memory Behavior Modeling","authors":"Xiaoxuan Shen, Zhihai Hu, Qirong Chen, Shengyingjie Liu, Ruxia Liang, Jianwen Sun","journal":"ArXiv","year":"2023","doi":"2408.14492v1","url":"https://arxiv.org/html/2408.14492v1","keyAssumptions":"Memory equations have fixed descriptors; Pure mathematical or data-driven approaches are optimal; Classical memory theories cannot be improved through ML","citation":"Shen, X., Hu, Z., Chen, Q., Liu, S., Liang, R., & Sun, J. (2023). Evolvable Psychology Informed Neural Network for Memory Behavior Modeling. arXiv preprint arXiv:2408.14492.","notes":"Combines neural networks with differentiating sparse regression to evolve memory equation descriptors. Addresses controversies in memory equation formulation. Shows AI can enhance traditional psychological models.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025081100007","title":"Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review","authors":"Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong","journal":"ArXiv","year":"2025","doi":"2409.06131v2","url":"https://arxiv.org/html/2409.06131v2","keyAssumptions":"Random data sampling optimal for LLM training; All data points equally valuable; Forgetting purely detrimental; Human learning techniques don't apply to large-scale training","citation":"Prakriya, N., Yen, J-N., Hsieh, C-J., & Cong, J. (2025). Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review. arXiv preprint arXiv:2409.06131.","notes":"Learn-Focus-Review paradigm achieves lower perplexity using only 5%-19% of training tokens. Matches 2× parameter models with 3.2% tokens. First massive-scale validation of spaced repetition in LLM training.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100008","title":"Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing","authors":"Grey Kuling, Marinka Zitnik","journal":"ArXiv","year":"2025","doi":"2507.00032","url":"https://arxiv.org/abs/2507.00032","keyAssumptions":"Knowledge tracing requires extensive cohort data; Gradient-based learning with backprop necessary; Memory should preserve all information; Biological principles don't scale","citation":"Kuling, G., & Zitnik, M. (2025). Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing. arXiv preprint arXiv:2507.00032.","notes":"Biologically inspired architecture with time-decaying Hebbian memory. Enables few-shot personalization. 1.75× faster training, 99.01% less memory usage. Validated in classroom deployment.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100009","title":"Adaptive Forgetting Curves for Spaced Repetition Language Learning","authors":"Francisco J. Valverde-Albacete, Carmen Peláez-Moreno","journal":"PMC","year":"2020","doi":"PMC7334729","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC7334729/","keyAssumptions":"Universal forgetting curves apply to all learners; Simple temporal spacing sufficient; Word difficulty uniform; Neural networks cannot capture psychological patterns","citation":"Valverde-Albacete, F. J., & Peláez-Moreno, C. (2020). Adaptive Forgetting Curves for Spaced Repetition Language Learning. PMC. doi: PMC7334729","notes":"Word complexity highly informative feature learned by neural networks. 4.28M learner-word datapoints from Duolingo. Demonstrates neural networks can effectively model psychological forgetting patterns.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100010","title":"DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Schedules","authors":"Jing Wang, Qinfeng Xiao","journal":"Applied Sciences","year":"2024","doi":"10.3390/app14135591","url":"https://www.mdpi.com/2076-3417/14/13/5591","keyAssumptions":"Handcrafted spacing rules sufficient; DRL should focus on item selection; One item per day constraint; Simple temporal intervals adequate","citation":"Wang, J., et al. (2024). DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Schedules. Applied Sciences, 14(13), 5591.","notes":"Focus on optimal intervals vs. item selection. 64% error reduction, 17% cost reduction. 220M row dataset with time-series information. Transformer-based half-life regression.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100011","title":"Forgetting in Machine Learning and Beyond: A Survey","authors":"Alyssa Shuang Sha, Bernardo Pereira Nunes, Armin Haller","journal":"ArXiv","year":"2024","doi":"2405.20620","url":"https://arxiv.org/abs/2405.20620","keyAssumptions":"Forgetting purely detrimental in ML; Memory should preserve all information; Human forgetting mechanisms don't apply to AI; Catastrophic forgetting unsolvable flaw","citation":"Sha, A. S., Nunes, B. P., & Haller, A. (2024). Forgetting in Machine Learning and Beyond: A Survey. arXiv preprint arXiv:2405.20620.","notes":"Comprehensive survey showing forgetting as adaptive function. Benefits across ML subfields: performance improvement, overfitting prevention, data privacy. Paradigm shift from problem to solution.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100012","title":"Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning","authors":"Jeremi I. Kaczmarek, Jakub Pokrywka, Krzysztof Biedalak, Grzegorz Kurzyp, Łukasz Grzybowski","journal":"ArXiv","year":"2025","doi":"2503.01859","url":"https://arxiv.org/html/2503.01859v1","keyAssumptions":"Manual medical content creation sufficient; General LMs adequate for medical content; Spaced repetition independent of content quality; Educational resources need extensive human curation","citation":"Kaczmarek, J. I., Pokrywka, J., Biedalak, K., Kurzyp, G., & Grzybowski, Ł. (2025). Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning. arXiv preprint arXiv:2503.01859.","notes":"RAG + spaced repetition integration for Polish medical education. Demonstrates content quality amplifies spaced repetition effectiveness. Medical expert validation of AI-generated content.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100013","title":"Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation","authors":"Guanglong Sun et al.","journal":"ArXiv","year":"2025","doi":"2502.06192","url":"https://arxiv.org/abs/2502.06192","keyAssumptions":"Continuous knowledge distillation optimal; Timing doesn't impact transfer; Biological principles don't apply to NNs; KD methods reached performance limits","citation":"Sun, G., et al. (2025). Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation. arXiv preprint arXiv:2502.06192.","notes":"Spaced Knowledge Distillation improves generalization through strategic timing. 2.31-3.34% improvements. Biological spacing effect validated in neural network training.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100014","title":"Task Scheduling & Forgetting in Multi-Task Reinforcement Learning","authors":"Marc Speckmann, Theresa Eimer","journal":"ArXiv","year":"2025","doi":"2503.01941","url":"https://arxiv.org/html/2503.01941v1","keyAssumptions":"Performance metrics sufficient for RL curricula; RL and human forgetting fundamentally different; Reactive forgetting approaches adequate; Human methods transfer directly to RL","citation":"Speckmann, M., & Eimer, T. (2025). Task Scheduling & Forgetting in Multi-Task Reinforcement Learning. arXiv preprint arXiv:2503.01941.","notes":"RL agents show human-like forgetting curves but need specialized scheduling. Leitner/SuperMemo don't transfer directly due to asymmetrical learning patterns.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100015","title":"Time-dependent consolidation mechanisms of durable memory in spaced learning","authors":"Dazhi Yin et al.","journal":"Nature Communications Biology","year":"2025","doi":"s42003-025-07964-6","url":"https://www.nature.com/articles/s42003-025-07964-6","keyAssumptions":"Hippocampus primary site for spacing benefits; Neural integration mechanisms unknown; Similar consolidation in spaced/massed learning; DMN role unclear","citation":"Yin, D., et al. (2025). Time-dependent consolidation mechanisms of durable memory in spaced learning. Communications Biology, 8, 123.","notes":"DMN integration, not hippocampal consolidation, drives spacing effect durability. Neural validation of distributed memory models for algorithm design.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100016","title":"Algorithmic Spaced Retrieval Enhances Long-Term Memory in Alzheimer Disease: Case-Control Pilot Study","authors":"Amy M Smith, Anna Marin, Renee E DeCaro, Richard Feinn, Audrey Wack, Gregory I Hughes, Nathaniel Rivard, Akshay Umashankar, Katherine W Turk, Andrew E Budson","journal":"JMIR Formative Research","year":"2024","doi":"10.2196/51943","url":"https://formative.jmir.org/2024/1/e51943","keyAssumptions":"Spaced retrieval difficult for AD patients; Manual scheduling sufficient; Fixed intervals work across impairment levels; Clinical interventions don't need algorithmic optimization","citation":"Smith, A. M., Marin, A., DeCaro, R. E., Feinn, R., Wack, A., Hughes, G. I., ... & Budson, A. E. (2024). Algorithmic Spaced Retrieval Enhances Long-Term Memory in Alzheimer Disease: Case-Control Pilot Study. JMIR Formative Research, 8, e51943.","notes":"ML-optimized spaced retrieval mobile app for MCI patients. Personalized forgetting rate tracking enables clinical translation. Demonstrates therapeutic potential.","addedDate":"2025-08-11T18:59:00Z"}