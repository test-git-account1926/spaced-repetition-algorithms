{"id":"paper_2025080900001","title":"LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning","authors":"Jiahao Zhao","journal":"ArXiv","year":"2024","doi":"2508.03275v1","url":"https://arxiv.org/html/2508.03275v1","keyAssumptions":"Items can be scheduled independently without semantic relationships; Temporal spacing alone is sufficient for optimal learning; Traditional algorithms work across all domains","citation":"Zhao, J. (2024). LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning. arXiv preprint arXiv:2508.03275.","notes":"First algorithm addressing semantic interference through LLM-powered similarity assessment. Achieves 90.2% vs 88.4% success rate. Addresses critical semantic confusion in vocabulary learning.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900002","title":"Human-like Forgetting Curves in Deep Neural Networks","authors":"Dylan Kline","journal":"ArXiv","year":"2025","doi":"2506.12034v2","url":"https://arxiv.org/abs/2506.12034","keyAssumptions":"Neural network forgetting is fundamentally different from human forgetting; Catastrophic forgetting requires specialized architectures; Human memory research has limited AI applicability","citation":"Kline, D. (2025). Human-like Forgetting Curves in Deep Neural Networks. arXiv preprint arXiv:2506.12034.","notes":"Demonstrates neural networks exhibit human-like exponential forgetting patterns. Enables direct application of spaced repetition to mitigate catastrophic forgetting. Bridges cognitive science and AI.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900003","title":"Optimizing Human Learning","authors":"Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schölkopf, Manuel Gomez-Rodriguez","journal":"ArXiv","year":"2017","doi":"1712.01856","url":"https://ar5iv.labs.arxiv.org/html/1712.01856","keyAssumptions":"Heuristic scheduling rules are sufficient; Optimal scheduling cannot be derived mathematically; Memory models too complex for optimization","citation":"Tabibian, B., Upadhyay, U., De, A., Zarezade, A., Schölkopf, B., & Gomez-Rodriguez, M. (2017). Optimizing human learning. arXiv preprint arXiv:1712.01856.","notes":"First theoretical proof that optimal spaced repetition schedules are determined by recall probability. Uses marked temporal point processes framework. Validated on Duolingo data.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900004","title":"Algorithm SM-18","authors":"Piotr Wozniak","journal":"SuperMemo.guru","year":"2019","doi":"","url":"https://supermemo.guru/wiki/Algorithm_SM-18","keyAssumptions":"Item difficulty is constant throughout learning; Single memory strength parameter sufficient; Fixed mathematical formulas for intervals","citation":"Wozniak, P. (2019). Algorithm SM-18. SuperMemo.guru. Retrieved from https://supermemo.guru/wiki/Algorithm_SM-18","notes":"Latest SuperMemo algorithm with dynamic difficulty estimation. Replaces assumption of constant item difficulty with adaptive modeling. Part of 34-year algorithm evolution achieving 1.1 to 35.3 improvement ratio over SM-2.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900005","title":"Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students","authors":"Prital Bamnodkar","journal":"ArXiv","year":"2025","doi":"2507.21109","url":"https://arxiv.org/html/2507.21109","keyAssumptions":"Experience replay alone sufficient for continual learning; Random memory sampling is optimal; Human learning strategies don't apply to neural networks","citation":"Bamnodkar, P. (2025). Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students. arXiv preprint arXiv:2507.21109.","notes":"Applies Active Recall, Deliberate Practice, and Spaced Repetition to continual learning. Achieves 13.17% vs 7.40% on Split CIFAR-100. Demonstrates human learning principles enhance neural network training.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025080900006","title":"Evolvable Psychology Informed Neural Network for Memory Behavior Modeling","authors":"Xiaoxuan Shen, Zhihai Hu, Qirong Chen, Shengyingjie Liu, Ruxia Liang, Jianwen Sun","journal":"ArXiv","year":"2023","doi":"2408.14492v1","url":"https://arxiv.org/html/2408.14492v1","keyAssumptions":"Memory equations have fixed descriptors; Pure mathematical or data-driven approaches are optimal; Classical memory theories cannot be improved through ML","citation":"Shen, X., Hu, Z., Chen, Q., Liu, S., Liang, R., & Sun, J. (2023). Evolvable Psychology Informed Neural Network for Memory Behavior Modeling. arXiv preprint arXiv:2408.14492.","notes":"Combines neural networks with differentiating sparse regression to evolve memory equation descriptors. Addresses controversies in memory equation formulation. Shows AI can enhance traditional psychological models.","addedDate":"2025-08-09T20:57:00Z"}
{"id":"paper_2025081100007","title":"Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review","authors":"Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong","journal":"ArXiv","year":"2025","doi":"2409.06131v2","url":"https://arxiv.org/html/2409.06131v2","keyAssumptions":"Random data sampling optimal for LLM training; All data points equally valuable; Forgetting purely detrimental; Human learning techniques don't apply to large-scale training","citation":"Prakriya, N., Yen, J-N., Hsieh, C-J., & Cong, J. (2025). Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review. arXiv preprint arXiv:2409.06131.","notes":"Learn-Focus-Review paradigm achieves lower perplexity using only 5%-19% of training tokens. Matches 2× parameter models with 3.2% tokens. First massive-scale validation of spaced repetition in LLM training.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100008","title":"Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing","authors":"Grey Kuling, Marinka Zitnik","journal":"ArXiv","year":"2025","doi":"2507.00032","url":"https://arxiv.org/abs/2507.00032","keyAssumptions":"Knowledge tracing requires extensive cohort data; Gradient-based learning with backprop necessary; Memory should preserve all information; Biological principles don't scale","citation":"Kuling, G., & Zitnik, M. (2025). Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing. arXiv preprint arXiv:2507.00032.","notes":"Biologically inspired architecture with time-decaying Hebbian memory. Enables few-shot personalization. 1.75× faster training, 99.01% less memory usage. Validated in classroom deployment.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100009","title":"Adaptive Forgetting Curves for Spaced Repetition Language Learning","authors":"Francisco J. Valverde-Albacete, Carmen Peláez-Moreno","journal":"PMC","year":"2020","doi":"PMC7334729","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC7334729/","keyAssumptions":"Universal forgetting curves apply to all learners; Simple temporal spacing sufficient; Word difficulty uniform; Neural networks cannot capture psychological patterns","citation":"Valverde-Albacete, F. J., & Peláez-Moreno, C. (2020). Adaptive Forgetting Curves for Spaced Repetition Language Learning. PMC. doi: PMC7334729","notes":"Word complexity highly informative feature learned by neural networks. 4.28M learner-word datapoints from Duolingo. Demonstrates neural networks can effectively model psychological forgetting patterns.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100010","title":"DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Schedules","authors":"Jing Wang, Qinfeng Xiao","journal":"Applied Sciences","year":"2024","doi":"10.3390/app14135591","url":"https://www.mdpi.com/2076-3417/14/13/5591","keyAssumptions":"Handcrafted spacing rules sufficient; DRL should focus on item selection; One item per day constraint; Simple temporal intervals adequate","citation":"Wang, J., et al. (2024). DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Schedules. Applied Sciences, 14(13), 5591.","notes":"Focus on optimal intervals vs. item selection. 64% error reduction, 17% cost reduction. 220M row dataset with time-series information. Transformer-based half-life regression.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100011","title":"Forgetting in Machine Learning and Beyond: A Survey","authors":"Alyssa Shuang Sha, Bernardo Pereira Nunes, Armin Haller","journal":"ArXiv","year":"2024","doi":"2405.20620","url":"https://arxiv.org/abs/2405.20620","keyAssumptions":"Forgetting purely detrimental in ML; Memory should preserve all information; Human forgetting mechanisms don't apply to AI; Catastrophic forgetting unsolvable flaw","citation":"Sha, A. S., Nunes, B. P., & Haller, A. (2024). Forgetting in Machine Learning and Beyond: A Survey. arXiv preprint arXiv:2405.20620.","notes":"Comprehensive survey showing forgetting as adaptive function. Benefits across ML subfields: performance improvement, overfitting prevention, data privacy. Paradigm shift from problem to solution.","addedDate":"2025-08-11T18:36:00Z"}
{"id":"paper_2025081100012","title":"Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning","authors":"Jeremi I. Kaczmarek, Jakub Pokrywka, Krzysztof Biedalak, Grzegorz Kurzyp, Łukasz Grzybowski","journal":"ArXiv","year":"2025","doi":"2503.01859","url":"https://arxiv.org/html/2503.01859v1","keyAssumptions":"Manual medical content creation sufficient; General LMs adequate for medical content; Spaced repetition independent of content quality; Educational resources need extensive human curation","citation":"Kaczmarek, J. I., Pokrywka, J., Biedalak, K., Kurzyp, G., & Grzybowski, Ł. (2025). Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning. arXiv preprint arXiv:2503.01859.","notes":"RAG + spaced repetition integration for Polish medical education. Demonstrates content quality amplifies spaced repetition effectiveness. Medical expert validation of AI-generated content.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100013","title":"Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation","authors":"Guanglong Sun et al.","journal":"ArXiv","year":"2025","doi":"2502.06192","url":"https://arxiv.org/abs/2502.06192","keyAssumptions":"Continuous knowledge distillation optimal; Timing doesn't impact transfer; Biological principles don't apply to NNs; KD methods reached performance limits","citation":"Sun, G., et al. (2025). Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation. arXiv preprint arXiv:2502.06192.","notes":"Spaced Knowledge Distillation improves generalization through strategic timing. 2.31-3.34% improvements. Biological spacing effect validated in neural network training.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100014","title":"Task Scheduling & Forgetting in Multi-Task Reinforcement Learning","authors":"Marc Speckmann, Theresa Eimer","journal":"ArXiv","year":"2025","doi":"2503.01941","url":"https://arxiv.org/html/2503.01941v1","keyAssumptions":"Performance metrics sufficient for RL curricula; RL and human forgetting fundamentally different; Reactive forgetting approaches adequate; Human methods transfer directly to RL","citation":"Speckmann, M., & Eimer, T. (2025). Task Scheduling & Forgetting in Multi-Task Reinforcement Learning. arXiv preprint arXiv:2503.01941.","notes":"RL agents show human-like forgetting curves but need specialized scheduling. Leitner/SuperMemo don't transfer directly due to asymmetrical learning patterns.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100015","title":"Time-dependent consolidation mechanisms of durable memory in spaced learning","authors":"Dazhi Yin et al.","journal":"Nature Communications Biology","year":"2025","doi":"s42003-025-07964-6","url":"https://www.nature.com/articles/s42003-025-07964-6","keyAssumptions":"Hippocampus primary site for spacing benefits; Neural integration mechanisms unknown; Similar consolidation in spaced/massed learning; DMN role unclear","citation":"Yin, D., et al. (2025). Time-dependent consolidation mechanisms of durable memory in spaced learning. Communications Biology, 8, 123.","notes":"DMN integration, not hippocampal consolidation, drives spacing effect durability. Neural validation of distributed memory models for algorithm design.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100016","title":"Algorithmic Spaced Retrieval Enhances Long-Term Memory in Alzheimer Disease: Case-Control Pilot Study","authors":"Amy M Smith, Anna Marin, Renee E DeCaro, Richard Feinn, Audrey Wack, Gregory I Hughes, Nathaniel Rivard, Akshay Umashankar, Katherine W Turk, Andrew E Budson","journal":"JMIR Formative Research","year":"2024","doi":"10.2196/51943","url":"https://formative.jmir.org/2024/1/e51943","keyAssumptions":"Spaced retrieval difficult for AD patients; Manual scheduling sufficient; Fixed intervals work across impairment levels; Clinical interventions don't need algorithmic optimization","citation":"Smith, A. M., Marin, A., DeCaro, R. E., Feinn, R., Wack, A., Hughes, G. I., ... & Budson, A. E. (2024). Algorithmic Spaced Retrieval Enhances Long-Term Memory in Alzheimer Disease: Case-Control Pilot Study. JMIR Formative Research, 8, e51943.","notes":"ML-optimized spaced retrieval mobile app for MCI patients. Personalized forgetting rate tracking enables clinical translation. Demonstrates therapeutic potential.","addedDate":"2025-08-11T18:59:00Z"}
{"id":"paper_2025081100017","title":"Do Your Best and Get Enough Rest for Continual Learning","authors":"Hankyul Kang, Gregor Seifer, Donghyun Lee, Jongbin Ryu","journal":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","year":"2025","doi":"2503.18371","url":"https://arxiv.org/abs/2503.18371","keyAssumptions":"Continuous learning optimal; Rest periods unnecessary; Random memory sampling sufficient; Human learning principles don't apply to neural networks","citation":"Kang, H., Seifer, G., Lee, D., & Ryu, J. (2025). Do Your Best and Get Enough Rest for Continual Learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.","notes":"Direct application of Ebbinghaus forgetting curve theory to neural networks. View-batch model optimizes recall intervals. Validates biological spacing principles for artificial learning.","addedDate":"2025-08-11T19:30:00Z"}
{"id":"paper_2025081100018","title":"Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models","authors":"Istabrak Abbes, Gopeshh Subbaraj, Matthew Riemer, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, Irina Rish","journal":"ArXiv","year":"2025","doi":"2508.01908","url":"https://arxiv.org/abs/2508.01908","keyAssumptions":"Complete retraining necessary for LLMs; Experience replay prohibitive at scale; Gradient alignment doesn't scale; Continual learning methods don't work for LLMs","citation":"Abbes, I., Subbaraj, G., Riemer, M., Islah, N., Therien, B., Tabaru, T., ... & Rish, I. (2025). Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models. arXiv preprint arXiv:2508.01908.","notes":"Experience replay and gradient alignment effective at 100B+ token scale. Small replay rates more valuable than model size increases. Enables continual LLM training.","addedDate":"2025-08-11T19:30:00Z"}
{"id":"paper_2025081100019","title":"Knowledge Tracing in Programming Education Integrating Students' Questions","authors":"Doyoun Kim, Suin Kim, Yojan Jo","journal":"ArXiv","year":"2025","doi":"2502.10408","url":"https://arxiv.org/abs/2502.10408","keyAssumptions":"Performance data sufficient for KT; Student questions are noise; Programming KT same as other domains; Historical interactions alone adequate","citation":"Kim, D., Kim, S., & Jo, Y. (2025). Knowledge Tracing in Programming Education Integrating Students' Questions. arXiv preprint arXiv:2502.10408.","notes":"SQKT integrates student questions as KT signals. 33.1% absolute AUC improvement. Semantic understanding enhances performance-based models.","addedDate":"2025-08-11T19:30:00Z"}
{"id":"paper_2025081100020","title":"Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems","authors":"Zhangqi Duan et al.","journal":"ArXiv","year":"2025","doi":"2502.18632","url":"https://arxiv.org/abs/2502.18632","keyAssumptions":"Human experts required for KC generation; Automated approaches inferior; Manual KC crafting necessary; Human-written KCs inherently superior","citation":"Duan, Z., et al. (2025). Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems. arXiv preprint arXiv:2502.18632.","notes":"LLM-based automated KC generation outperforms human experts. KCGen-KT framework eliminates manual bottleneck while exceeding human-level quality.","addedDate":"2025-08-11T19:30:00Z"}
{"id":"paper_2025081100021","title":"Content-aware Spaced Repetition","authors":"Giacomo Randazzo","journal":"Blog Post","year":"2025","doi":"","url":"https://www.giacomoran.com/blog/content-aware-sr/","keyAssumptions":"Memory models can operate using only performance ratings; Item independence assumption; Content semantics irrelevant to scheduling; Simple performance tracking sufficient","citation":"Randazzo, G. (2025). Content-aware Spaced Repetition. Retrieved from https://www.giacomoran.com/blog/content-aware-sr/","notes":"Introduces content-aware memory models using semantic meaning for scheduling. Separates schedulers from memory models. KARL prototype demonstrates feasibility.","addedDate":"2025-08-11T19:46:00Z"}
{"id":"paper_2025081100022","title":"Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall","authors":"Xuefei Hou, Xizhao Tan","journal":"ArXiv","year":"2025","doi":"2506.20156","url":"https://arxiv.org/abs/2506.20156","keyAssumptions":"Decontextualized review sufficient; Context-triggered retrieval unnecessary; Manual knowledge management scalable; Isolated flashcard review optimal","citation":"Hou, X., & Tan, X. (2025). Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall. arXiv preprint arXiv:2506.20156.","notes":"Context-triggered insight recall using JITAI framework. Dynamic knowledge graph with LLM similarity assessment. Prototype system for metacognitive scaffolding.","addedDate":"2025-08-11T19:46:00Z"}
{"id":"paper_2025081100023","title":"Memory3: Language Modeling with Explicit Memory","authors":"Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, et al.","journal":"ArXiv","year":"2024","doi":"2407.01178v1","url":"https://arxiv.org/html/2407.01178v1","keyAssumptions":"Knowledge must be stored primarily in model parameters; Working memory is the only alternative to parameter storage; Larger parameter counts necessary for better performance; RAG systems are primary solution for external knowledge","citation":"Yang, H., Lin, Z., Wang, W., Wu, H., et al. (2024). Memory3: Language Modeling with Explicit Memory. arXiv preprint arXiv:2407.01178.","notes":"Introduces explicit memory as third form of memory in LLMs. Achieves better performance than larger models with 2.4B parameters. Enables knowledge externalization reducing training and inference costs proportional to externalized knowledge.","addedDate":"2025-08-11T19:55:00Z"}
{"id":"paper_2025081100024","title":"Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph","authors":"Akash Vishwakarma, Hojin Lee, Mohith Suresh, Priyam Shankar Sharma, Rahul Vishwakarma, Sparsh Gupta, Yuvraj Anupam Chauhan","journal":"ArXiv","year":"2025","doi":"2506.08098","url":"https://arxiv.org/abs/2506.08098","keyAssumptions":"Memory systems function primarily as data storage repositories; Flat memory architectures sufficient for AI agents; Temporal dynamics not critical for memory organization; Raw interaction data adequate without synthesis","citation":"Vishwakarma, A., Lee, H., Suresh, M., Sharma, P. S., Vishwakarma, R., Gupta, S., & Chauhan, Y. A. (2025). Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph. arXiv preprint arXiv:2506.08098.","notes":"Introduces spatio-temporal resonance graph for dynamic knowledge synthesis. Achieves 34% improvement in task completion through insight aggregates and cognitive refinement. Autonomous higher-level knowledge generation from semantically rich insight particles.","addedDate":"2025-08-11T19:55:00Z"}
{"id":"paper_2025081100025","title":"Key-value Memory in the Brain","authors":"Samuel J. Gershman, Ila Fiete, Kazuki Irie","journal":"ArXiv","year":"2025","doi":"2501.02950v1","url":"https://arxiv.org/html/2501.02950v1","keyAssumptions":"Memory systems use same representation for storage and retrieval; Similarity-based retrieval is fundamental mechanism; Storage fidelity and retrieval discriminability cannot be simultaneously optimized; Brain memory follows simple associative models","citation":"Gershman, S. J., Fiete, I., & Irie, K. (2025). Key-value Memory in the Brain. arXiv preprint arXiv:2501.02950.","notes":"Proposes key-value memory architecture distinguishing storage (values) and retrieval (keys) representations. Enables simultaneous optimization for storage fidelity and retrieval discriminability. Bridges neuroscience with modern ML memory systems like transformers.","addedDate":"2025-08-11T19:55:00Z"}
{"id":"paper_2025081100026","title":"Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length","authors":"Chupei Wang, Jiaqiu Vince Sun","journal":"ArXiv","year":"2025","doi":"2506.08184","url":"https://arxiv.org/abs/2506.08184","keyAssumptions":"Longer contexts always improve retrieval; Retrieval difficulty primarily determined by input length; LLM retrieval is mere lookup process; Context access is primary bottleneck","citation":"Wang, C., & Sun, J. V. (2025). Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length. arXiv preprint arXiv:2506.08184.","notes":"Introduces proactive interference paradigm from cognitive science. Reveals universal log-linear decline in retrieval accuracy as semantic interference accumulates. Identifies fundamental working memory constraints beyond context length limitations.","addedDate":"2025-08-11T20:04:00Z"}
{"id":"paper_2025081100027","title":"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge Editing for Large Language Models","authors":"Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","journal":"ArXiv","year":"2024","doi":"2408.07413","url":"https://arxiv.org/abs/2408.07413","keyAssumptions":"Knowledge editing can scale to lifelong learning; Knowledge representations are independent; Linear associative memory models sufficient; Editing failures due to implementation issues","citation":"Hu, C., Cao, P., Chen, Y., Liu, K., & Zhao, J. (2024). Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge Editing for Large Language Models. arXiv preprint arXiv:2408.07413.","notes":"Mathematical proof that knowledge superposition causes fundamental interference in lifelong editing. Knowledge superposition exhibits universal statistical properties across models. Provides theoretical foundation for understanding neural network memory limitations.","addedDate":"2025-08-11T20:04:00Z"}
{"id":"paper_2025081100028","title":"PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes","authors":"Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang","journal":"ArXiv","year":"2025","doi":"2507.04607","url":"https://arxiv.org/abs/2507.04607","keyAssumptions":"LLM personalization achievable through simple prompt engineering; Memory systems don't need cognitive foundations; Historical interactions can be treated uniformly; Personalization effectiveness determined by data quantity","citation":"Zhang, X. F., Beauchamp, N., & Wang, L. (2025). PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes. arXiv preprint arXiv:2507.04607.","notes":"Integrates dual-memory model from cognitive science into LLM personalization. Distinguishes episodic memory (historical interactions) from semantic memory (long-term beliefs). Augments with personalized thinking capability. First unified theoretical framework for LLM personalization.","addedDate":"2025-08-11T20:04:00Z"}
{"id":"paper_2025081100029","title":"TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation","authors":"Zhaoxing Li, Vahid Yazdanpanah, Jindi Wang, Wen Gu, Lei Shi, Alexandra I. Cristea, Sarah Kiden, Sebastian Stein","journal":"ArXiv","year":"2025","doi":"2502.15709","url":"https://arxiv.org/abs/2502.15709","keyAssumptions":"General-purpose LLMs sufficient for education; KT and LLMs are separate technologies; RAG systems don't need learning state information; Educational AI doesn't need real-time adaptation","citation":"Li, Z., Yazdanpanah, V., Wang, J., Gu, W., Shi, L., Cristea, A. I., Kiden, S., & Stein, S. (2025). TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation. arXiv preprint arXiv:2502.15709.","notes":"Novel combination of Knowledge Tracing, RAG, and LLMs for personalized education. MLFBK model predicts learning states, Scraper model enhances retrieval. 10% improvement in user satisfaction, 5% increase in quiz scores. First successful integration of multiple AI technologies for education.","addedDate":"2025-08-11T20:04:00Z"}
{"id":"paper_2025081100030","title":"In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents","authors":"Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister","journal":"ArXiv","year":"2025","doi":"2503.08026","url":"https://arxiv.org/abs/2503.08026","keyAssumptions":"Rigid memory granularity sufficient for conversation storage; Fixed retrieval mechanisms handle diverse contexts; LLMs maintain continuity through context windows; Memory systems don't need semantic structure understanding","citation":"Tan, Z., Yan, J., Hsu, I-H., Han, R., Wang, Z., Le, L. T., Song, Y., Chen, Y., Palangi, H., Lee, G., Iyer, A., Chen, T., Liu, H., Lee, C-Y., & Pfister, T. (2025). In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents. arXiv preprint arXiv:2503.08026.","notes":"Reflective Memory Management integrates flexible memory granularity with adaptive retrieval mechanisms. Captures natural semantic conversation structure. Enables sustained personalization through intelligent memory organization. Addresses fundamental LLM long-term memory limitations.","addedDate":"2025-08-11T20:04:00Z"}
{"id":"paper_2025081100031","title":"Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing","authors":"Yilmazcan Ozyurt et al.","journal":"ArXiv","year":"2025","doi":"2507.11060","url":"https://arxiv.org/abs/2507.11060","keyAssumptions":"Exercise recommendation can ignore semantic content; Sequential learning progression not crucial; Standard KT methods sufficient for recommendation; Q-learning adequate for educational RL","citation":"Ozyurt, Y., et al. (2025). Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing. arXiv preprint arXiv:2507.11060.","notes":"ExRec framework combines semantic KC representations with RL-based recommendation. Model-based Value Estimation (MVE) leverages KT components for cumulative knowledge improvement. Validates across four real-world math learning tasks with interpretable learning trajectories.","addedDate":"2025-08-11T21:18:00Z"}
{"id":"paper_2025081100032","title":"Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs","authors":"Alexander Scarlatos, Ryan S. Baker, Andrew Lan","journal":"LAK Conference","year":"2025","doi":"10.1145/3706468.3706501","url":"https://dl.acm.org/doi/10.1145/3706468.3706501","keyAssumptions":"Traditional KT methods work for open-ended dialogue; Manual dialogue analysis scalable; Knowledge components easily identifiable in conversations; Existing KT models handle dialogue context effectively","citation":"Scarlatos, A., Baker, R. S., & Lan, A. (2025). Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs. Proceedings of the 15th International Learning Analytics and Knowledge Conference.","notes":"LLMKT method uses LLM prompting to identify knowledge components in dialogue turns. Significantly outperforms existing KT methods in predicting student response correctness in dialogues. Addresses challenges in dialogue-based knowledge tracing for tutoring systems.","addedDate":"2025-08-11T21:18:00Z"}
{"id":"paper_2025081100033","title":"Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating","authors":"Yiyun Zhou, Zheqi Lv, Shengyu Zhang, Jingyuan Chen","journal":"ArXiv","year":"2025","doi":"2505.19543","url":"https://arxiv.org/abs/2505.19543","keyAssumptions":"Learner abilities remain stable over short periods; Predictable ability changes based on prior performance; Retraining necessary for adaptation; Model updates require parameter fine-tuning","citation":"Zhou, Y., Lv, Z., Zhang, S., & Chen, J. (2025). Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating. arXiv preprint arXiv:2505.19543.","notes":"Addresses Real-time Learning Pattern Adjustment (RLPA) caused by cognitive fatigue, motivation, external stress. Controller-generator architecture adapts to data changes without fine-tuning. Average 10% relative AUC increase for intra-learner shifts, 4% for inter-learner shifts with negligible time cost.","addedDate":"2025-08-11T21:18:00Z"}
{"id":"paper_2025081100034","title":"Control knowledge tracing: Modeling students' learning from control theory viewpoint","authors":"Haoxin Li et al.","journal":"Computers & Education: X-Reality","year":"2024","doi":"10.1016/j.cexr.2024.100095","url":"https://www.sciencedirect.com/science/article/pii/S2666920X2400095X","keyAssumptions":"Educational systems don't follow control theory principles; Student knowledge states can't be modeled as dynamic systems; Teaching optimization doesn't need systematic control approaches; Assessment prediction independent of learning resource modeling","citation":"Li, H., et al. (2024). Control knowledge tracing: Modeling students' learning from control theory viewpoint. Computers & Education: X-Reality.","notes":"CtrKT models student learning as control system with dynamic equation for knowledge state changes and observation equation mapping states to question scores. Enables knowledge state tracking, performance prediction, and teaching planning/optimization. Validated on psychology experiments and civil engineering course data.","addedDate":"2025-08-11T21:18:00Z"}
{"id":"paper_2025081100035","title":"Enhanced Learning Behaviors and Ability Knowledge Tracing for Personalized Learning","authors":"Unknown Authors","journal":"Applied Sciences","year":"2025","doi":"10.3390/app15020883","url":"https://www.mdpi.com/2076-3417/15/2/883","keyAssumptions":"Exercise sequences and answers sufficient for KT; Overall learning performance indicators irrelevant; Difficulty, timing, hints don't impact knowledge modeling; Static knowledge state representations adequate","citation":"Authors Unknown. (2025). Enhanced Learning Behaviors and Ability Knowledge Tracing for Personalized Learning. Applied Sciences, 15(2), 883.","notes":"Focuses on comprehensive learning performance including exercise difficulty, answer time, hint usage, and forgetting behavior. Addresses limitations of current KT methods that focus only on exercise sequences. Provides more comprehensive understanding of learning processes and capacity for knowledge absorption/retention.","addedDate":"2025-08-11T21:18:00Z"}
{"id":"paper_2025081100036","title":"CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models","authors":"Runze Li, Siyu Wu, Jun Wang, Wei Zhang","journal":"ArXiv","year":"2025","doi":"2505.17705","url":"https://arxiv.org/abs/2505.17705","keyAssumptions":"KT systems can operate with single-pass analysis; Prediction and analysis components should be independent; Direct LLM application sufficient; Explainability requires performance trade-offs","citation":"Li, R., Wu, S., Wang, J., & Zhang, W. (2025). CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models. arXiv preprint arXiv:2505.17705.","notes":"Dual-component architecture with Analyst generating explainable profiles and Predictor forecasting performance. Synergistic optimization loop improves both accuracy and explainability. Demonstrates collaborative AI architectures for educational applications.","addedDate":"2025-08-11T22:59:00Z"}
{"id":"paper_2025081100037","title":"Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing","authors":"Lixiang Xu, Xianwei Ding, Xin Yuan, Richang Hong, Feiping Nie, Enhong Chen, Philip S. Yu","journal":"ArXiv","year":"2025","doi":"2506.02949","url":"https://arxiv.org/abs/2506.02949","keyAssumptions":"Feature enhancement primary focus for KT improvement; Direct performance modeling without cognitive optimization; Static cognitive representations; Uniform processing of student interactions","citation":"Xu, L., Ding, X., Yuan, X., Hong, R., Nie, F., Chen, E., & Yu, P. S. (2025). Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing. arXiv preprint arXiv:2506.02949.","notes":"CRDP-KT uses dynamic programming to optimize cognitive representations based on question difficulty and performance intervals. Maintains cognitive continuity and coherence while minimizing non-cognitive interference. First systematic approach to cognitive representation optimization in KT.","addedDate":"2025-08-11T22:59:00Z"}
{"id":"paper_2025081100038","title":"Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education","authors":"Yiwei Sun","journal":"ArXiv","year":"2025","doi":"2506.00057","url":"https://arxiv.org/abs/2506.00057","keyAssumptions":"Complex models required for educational accuracy; Interpretability requires sacrificing statistical rigor; Uniform student treatment effective; Educator intuition sufficient for assessment","citation":"Sun, Y. (2025). Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education. arXiv preprint arXiv:2506.00057.","notes":"Demonstrates rigorous statistical approach with hierarchical Bayesian modeling providing both accuracy and interpretability. Identifies skill difficulty patterns and student subgroups in engineering education. Provides educators with actionable, data-informed insights without sacrificing predictive performance.","addedDate":"2025-08-11T22:59:00Z"}
{"id":"paper_2025081200039","title":"LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction","authors":"Ziwei Wang, Jie Zhou, Qin Chen, Min Zhang, Bo Jiang, Aimin Zhou, Qinchun Bai, Liang He","journal":"ArXiv","year":"2025","doi":"2502.02945","url":"https://arxiv.org/abs/2502.02945","keyAssumptions":"Sequence interaction models with ID-based information sufficient; Student patterns capturable without world knowledge; Traditional KT adequate without semantic reasoning","citation":"Wang, Z., Zhou, J., Chen, Q., Zhang, M., Jiang, B., Zhou, A., Bai, Q., & He, L. (2025). LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction. arXiv preprint arXiv:2502.02945.","notes":"Plug-and-Play instruction system aligns LLMs with KT through task-level and modality-level integration. Achieves state-of-the-art performance on 4 datasets against ~20 baselines. Demonstrates LLM world knowledge enhances student behavioral pattern modeling.","addedDate":"2025-08-12T07:34:00Z"}
{"id":"paper_2025081200040","title":"Personalized Student Knowledge Modeling for Future Learning Resource Prediction","authors":"Soroush Hashemifar, Sherry Sahebi","journal":"ArXiv","year":"2025","doi":"2505.14072","url":"https://arxiv.org/abs/2505.14072","keyAssumptions":"Fixed-size sequences adequate; Assessed materials sufficient; Knowledge and behavior independent; Uniform student treatment effective","citation":"Hashemifar, S., & Sahebi, S. (2025). Personalized Student Knowledge Modeling for Future Learning Resource Prediction. arXiv preprint arXiv:2505.14072.","notes":"KMaP framework uses clustering-based profiling for personalized representations. Multi-task learning for simultaneous knowledge tracing and behavior prediction. Includes non-assessed materials like lectures in modeling.","addedDate":"2025-08-12T07:34:00Z"}
{"id":"paper_2025081200041","title":"Survey of Loss Augmented Knowledge Tracing","authors":"Altun Shukurlu","journal":"ArXiv","year":"2025","doi":"2504.15163","url":"https://arxiv.org/abs/2504.15163","keyAssumptions":"Standard loss functions (cross-entropy, MSE) sufficient; Simple supervised learning adequate; Loss function design not primary factor; Contrastive learning not applicable to educational sequences","citation":"Shukurlu, A. (2025). Survey of Loss Augmented Knowledge Tracing. arXiv preprint arXiv:2504.15163.","notes":"Comprehensive survey of contrastive knowledge tracing algorithms (Bi-CLKT, CL4KT, SP-CLKT, CoSKT). Demonstrates significant performance improvements through advanced loss function design. Establishes loss optimization as fundamental KT research direction.","addedDate":"2025-08-12T07:34:00Z"}
{"id":"paper_2025081200042","title":"Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks","authors":"Naimul Haque","journal":"ArXiv","year":"2025","doi":"2504.01241","url":"https://arxiv.org/abs/2504.01241","keyAssumptions":"LLMs can adapt without significant forgetting; Models maintain knowledge across tasks; Catastrophic forgetting unavoidable in continual fine-tuning; All models exhibit similar forgetting patterns","citation":"Haque, N. (2025). Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks. arXiv preprint arXiv:2504.01241.","notes":"Evaluates continual fine-tuning on GLUE benchmark tasks for models under 10B parameters. Phi-3.5-mini shows minimal forgetting with strong learning. Orca-2-7b and Qwen2.5-7B demonstrate excellent adaptation through prompt engineering.","addedDate":"2025-08-12T07:34:00Z"}